{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import convergence as cr\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GloVe\n",
    "\n",
    "I used word vectors from a pre-trained set of data consisting of <a href=\"https://catalog.ldc.upenn.edu/LDC2011T07\">Gigawords</a> and <a href=\"https://dumps.wikimedia.org/backup-index.html\">2014 Wikimedia dumps</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def load_glove_model(file):\n",
    "#     print(\"Loading...\")\n",
    "#     f = open(file,'r')\n",
    "#     model = {}\n",
    "#     for line in f:\n",
    "#         whole_line = line.split()\n",
    "#         words = whole_line[0]\n",
    "#         embedding = np.array([float(val) for val in whole_line[1:]])\n",
    "#         model[words] = embedding\n",
    "#     print(\"Done.\",len(model),\" words loaded!\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# model = load_glove_model('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model['the'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for key, value in model.items():\n",
    "#     model[key] = value.reshape(1,-1)\n",
    "# model['the'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# # pickle the model\n",
    "# filename = 'pre_trained_model.pkl'\n",
    "# pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['the'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the GloVe model is pickled and loaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common English Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of 10,000 common english words from https://github.com/first20hours/google-10000-english. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('google-10000-english-usa.txt') as f:\n",
    "#     cwordlist = []\n",
    "#     for line in f:\n",
    "#         cwordlist.append(line.strip())\n",
    "# len(cwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #look at the word stems \n",
    "# wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grateful',\n",
       " 'emerald',\n",
       " 'gradually',\n",
       " 'laughing',\n",
       " 'grows',\n",
       " 'cliff',\n",
       " 'desirable',\n",
       " 'tract',\n",
       " 'ul',\n",
       " 'ballet',\n",
       " 'ol',\n",
       " 'journalist',\n",
       " 'abraham',\n",
       " 'j',\n",
       " 'bumper',\n",
       " 'afterwards',\n",
       " 'webpage',\n",
       " 'religion',\n",
       " 'garlic',\n",
       " 'hostel',\n",
       " 'shine',\n",
       " 'senegal',\n",
       " 'explosion',\n",
       " 'pn',\n",
       " 'banned',\n",
       " 'wendy',\n",
       " 'brief',\n",
       " 'signature',\n",
       " 'diffs',\n",
       " 'cove',\n",
       " 'mumbai',\n",
       " 'ozone',\n",
       " 'discipline',\n",
       " 'casa',\n",
       " 'mu',\n",
       " 'daughter',\n",
       " 'conversation',\n",
       " 'radio',\n",
       " 'tariff',\n",
       " 'nvidia',\n",
       " 'opponent',\n",
       " 'pasta',\n",
       " 'simplified',\n",
       " 'muscle',\n",
       " 'serum',\n",
       " 'wrapped',\n",
       " 'swift',\n",
       " 'motherboard',\n",
       " 'runtime',\n",
       " 'inbox',\n",
       " 'focal',\n",
       " 'bibliographic',\n",
       " 'vagina',\n",
       " 'eden',\n",
       " 'distant',\n",
       " 'incl',\n",
       " 'champagne',\n",
       " 'ala',\n",
       " 'decimal',\n",
       " 'hq',\n",
       " 'deviation',\n",
       " 'superintendent',\n",
       " 'propecia',\n",
       " 'dip',\n",
       " 'nbc',\n",
       " 'samba',\n",
       " 'hostel',\n",
       " 'housewife',\n",
       " 'employ',\n",
       " 'mongolia',\n",
       " 'penguin',\n",
       " 'magical',\n",
       " 'influence',\n",
       " 'inspection',\n",
       " 'irrigation',\n",
       " 'miracle',\n",
       " 'manually',\n",
       " 'reprint',\n",
       " 'reid',\n",
       " 'wt',\n",
       " 'hydraulic',\n",
       " 'centered',\n",
       " 'robertson',\n",
       " 'flex',\n",
       " 'yearly',\n",
       " 'penetration',\n",
       " 'wound',\n",
       " 'belle',\n",
       " 'rosa',\n",
       " 'conviction',\n",
       " 'hash',\n",
       " 'omission',\n",
       " 'writing',\n",
       " 'hamburg',\n",
       " 'lazy',\n",
       " 'mv',\n",
       " 'mpg',\n",
       " 'retrieval',\n",
       " 'quality',\n",
       " 'cindy',\n",
       " 'lolita',\n",
       " 'father',\n",
       " 'carb',\n",
       " 'charging',\n",
       " 'ca',\n",
       " 'marvel',\n",
       " 'lined',\n",
       " 'cio',\n",
       " 'dow',\n",
       " 'prototype',\n",
       " 'importantly',\n",
       " 'rb',\n",
       " 'petite',\n",
       " 'apparatus',\n",
       " 'upc',\n",
       " 'terrain',\n",
       " 'duo',\n",
       " 'pen',\n",
       " 'explaining',\n",
       " 'yen',\n",
       " 'strip',\n",
       " 'gossip',\n",
       " 'ranger',\n",
       " 'nomination',\n",
       " 'empirical',\n",
       " 'mh',\n",
       " 'rotary',\n",
       " 'worm',\n",
       " 'dependence',\n",
       " 'discrete',\n",
       " 'beginner',\n",
       " 'boxed',\n",
       " 'lid',\n",
       " 'sexuality',\n",
       " 'polyester',\n",
       " 'cubic',\n",
       " 'deaf',\n",
       " 'commitment',\n",
       " 'suggesting',\n",
       " 'sapphire',\n",
       " 'kinase',\n",
       " 'skirt',\n",
       " 'mat',\n",
       " 'remainder',\n",
       " 'crawford',\n",
       " 'labeled',\n",
       " 'privilege',\n",
       " 'television',\n",
       " 'specializing',\n",
       " 'marking',\n",
       " 'commodity',\n",
       " 'pvc',\n",
       " 'serbia',\n",
       " 'sheriff',\n",
       " 'griffin',\n",
       " 'declined',\n",
       " 'guyana',\n",
       " 'spy',\n",
       " 'blah',\n",
       " 'mime',\n",
       " 'neighbor',\n",
       " 'motorcycle',\n",
       " 'elect',\n",
       " 'highway',\n",
       " 'thinkpad',\n",
       " 'concentrate',\n",
       " 'intimate',\n",
       " 'reproductive',\n",
       " 'preston',\n",
       " 'deadly',\n",
       " 'cunt',\n",
       " 'feof',\n",
       " 'bunny',\n",
       " 'chevy',\n",
       " 'molecule',\n",
       " 'round',\n",
       " 'longest',\n",
       " 'refrigerator',\n",
       " 'tions',\n",
       " 'interval',\n",
       " 'sentence',\n",
       " 'dentist',\n",
       " 'usda',\n",
       " 'exclusion',\n",
       " 'workstation',\n",
       " 'holocaust',\n",
       " 'keen',\n",
       " 'flyer',\n",
       " 'pea',\n",
       " 'dosage',\n",
       " 'receiver',\n",
       " 'url',\n",
       " 'customize',\n",
       " 'disposition',\n",
       " 'variance',\n",
       " 'navigator',\n",
       " 'investigator',\n",
       " 'cameroon',\n",
       " 'baking',\n",
       " 'marijuana',\n",
       " 'adaptive',\n",
       " 'computed',\n",
       " 'needle',\n",
       " 'bath',\n",
       " 'enb',\n",
       " 'gg',\n",
       " 'cathedral',\n",
       " 'brake',\n",
       " 'og',\n",
       " 'nirvana',\n",
       " 'ko',\n",
       " 'fairfield',\n",
       " 'owns',\n",
       " 'til',\n",
       " 'invision',\n",
       " 'sticky',\n",
       " 'destiny',\n",
       " 'generous',\n",
       " 'madness',\n",
       " 'emacs',\n",
       " 'climb',\n",
       " 'blowing',\n",
       " 'fascinating',\n",
       " 'landscape',\n",
       " 'heated',\n",
       " 'lafayette',\n",
       " 'jackie',\n",
       " 'wto',\n",
       " 'computation',\n",
       " 'hay',\n",
       " 'cardiovascular',\n",
       " 'ww',\n",
       " 'sparc',\n",
       " 'cardiac',\n",
       " 'salvation',\n",
       " 'dover',\n",
       " 'adrian',\n",
       " 'prediction',\n",
       " 'accompanying',\n",
       " 'vatican',\n",
       " 'brutal',\n",
       " 'learner',\n",
       " 'gd',\n",
       " 'selective',\n",
       " 'arbitration',\n",
       " 'configuring',\n",
       " 'token',\n",
       " 'editorial',\n",
       " 'zinc',\n",
       " 'sacrifice',\n",
       " 'seeker',\n",
       " 'guru',\n",
       " 'isa',\n",
       " 'removable',\n",
       " 'convergence',\n",
       " 'yield',\n",
       " 'gibraltar',\n",
       " 'levy',\n",
       " 'suited',\n",
       " 'numeric',\n",
       " 'anthropology',\n",
       " 'skating',\n",
       " 'kinda',\n",
       " 'aberdeen',\n",
       " 'emperor',\n",
       " 'grad',\n",
       " 'malpractice',\n",
       " 'dylan',\n",
       " 'bra',\n",
       " 'belt',\n",
       " 'black',\n",
       " 'educated',\n",
       " 'rebate',\n",
       " 'reporter',\n",
       " 'burke',\n",
       " 'proudly',\n",
       " 'pix',\n",
       " 'necessity',\n",
       " 'rendering',\n",
       " 'mic',\n",
       " 'inserted',\n",
       " 'pulling',\n",
       " 'basename',\n",
       " 'kyle',\n",
       " 'obesity',\n",
       " 'curve',\n",
       " 'suburban',\n",
       " 'touring',\n",
       " 'clara',\n",
       " 'vertex',\n",
       " 'bw',\n",
       " 'hepatitis',\n",
       " 'nationally',\n",
       " 'tomato',\n",
       " 'andorra',\n",
       " 'waterproof',\n",
       " 'expired',\n",
       " 'mj',\n",
       " 'travel',\n",
       " 'flush',\n",
       " 'waiver',\n",
       " 'pale',\n",
       " 'specialty',\n",
       " 'hayes',\n",
       " 'humanitarian',\n",
       " 'invitation',\n",
       " 'functioning',\n",
       " 'delight',\n",
       " 'survivor',\n",
       " 'garcia',\n",
       " 'cingular',\n",
       " 'economy',\n",
       " 'alexandria',\n",
       " 'bacterial',\n",
       " 'moses',\n",
       " 'counted',\n",
       " 'undertake',\n",
       " 'declare',\n",
       " 'continuously',\n",
       " 'john',\n",
       " 'valve',\n",
       " 'gap',\n",
       " 'impaired',\n",
       " 'achievement',\n",
       " 'donor',\n",
       " 'tear',\n",
       " 'jewel',\n",
       " 'teddy',\n",
       " 'lf',\n",
       " 'convertible',\n",
       " 'ata',\n",
       " 'teach',\n",
       " 'venture',\n",
       " 'nil',\n",
       " 'bufing',\n",
       " 'stranger',\n",
       " 'tragedy',\n",
       " 'julian',\n",
       " 'nest',\n",
       " 'pam',\n",
       " 'dryer',\n",
       " 'painful',\n",
       " 'velvet',\n",
       " 'tribunal',\n",
       " 'ruled',\n",
       " 'nato',\n",
       " 'pension',\n",
       " 'prayer',\n",
       " 'funky',\n",
       " 'secretariat',\n",
       " 'nowhere',\n",
       " 'cop',\n",
       " 'paragraph',\n",
       " 'gale',\n",
       " 'join',\n",
       " 'adolescent',\n",
       " 'nomination',\n",
       " 'wesley',\n",
       " 'dim',\n",
       " 'lately',\n",
       " 'cancelled',\n",
       " 'scary',\n",
       " 'mattress',\n",
       " 'mpeg',\n",
       " 'brunei',\n",
       " 'likewise',\n",
       " 'banana',\n",
       " 'introductory',\n",
       " 'slovak',\n",
       " 'cake',\n",
       " 'stan',\n",
       " 'reservoir',\n",
       " 'occurrence',\n",
       " 'idol',\n",
       " 'bloody',\n",
       " 'mixer',\n",
       " 'remind',\n",
       " 'wc',\n",
       " 'worcester',\n",
       " 'sbjct',\n",
       " 'demographic',\n",
       " 'charming',\n",
       " 'mai',\n",
       " 'tooth',\n",
       " 'disciplinary',\n",
       " 'annoying',\n",
       " 'respected',\n",
       " 'stay',\n",
       " 'disclose',\n",
       " 'affair',\n",
       " 'drove',\n",
       " 'washer',\n",
       " 'upset',\n",
       " 'restrict',\n",
       " 'springer',\n",
       " 'beside',\n",
       " 'mine',\n",
       " 'portrait',\n",
       " 'rebound',\n",
       " 'logan',\n",
       " 'mentor',\n",
       " 'interpreted',\n",
       " 'evaluation',\n",
       " 'fought',\n",
       " 'baghdad',\n",
       " 'elimination',\n",
       " 'metre',\n",
       " 'hypothetical',\n",
       " 'immigrant',\n",
       " 'complimentary',\n",
       " 'helicopter',\n",
       " 'pencil',\n",
       " 'freeze',\n",
       " 'hk',\n",
       " 'performer',\n",
       " 'abu',\n",
       " 'titled',\n",
       " 'commission',\n",
       " 'sphere',\n",
       " 'powerseller',\n",
       " 'moss',\n",
       " 'ratio',\n",
       " 'concord',\n",
       " 'graduated',\n",
       " 'endorsed',\n",
       " 'ty',\n",
       " 'surprising',\n",
       " 'walnut',\n",
       " 'lance',\n",
       " 'ladder',\n",
       " 'italia',\n",
       " 'unnecessary',\n",
       " 'dramatically',\n",
       " 'liberia',\n",
       " 'sherman',\n",
       " 'cork',\n",
       " 'maximize',\n",
       " 'cj',\n",
       " 'hansen',\n",
       " 'senator',\n",
       " 'workout',\n",
       " 'mali',\n",
       " 'yugoslavia',\n",
       " 'bleeding',\n",
       " 'characterization',\n",
       " 'colon',\n",
       " 'likelihood',\n",
       " 'lane',\n",
       " 'purse',\n",
       " 'fundamental',\n",
       " 'contamination',\n",
       " 'mtv',\n",
       " 'endangered',\n",
       " 'compromise',\n",
       " 'masturbation',\n",
       " 'optimize',\n",
       " 'stating',\n",
       " 'dome',\n",
       " 'caroline',\n",
       " 'leu',\n",
       " 'expiration',\n",
       " 'namespace',\n",
       " 'align',\n",
       " 'peripheral',\n",
       " 'bless',\n",
       " 'engaging',\n",
       " 'negotiation',\n",
       " 'crest',\n",
       " 'opponent',\n",
       " 'triumph',\n",
       " 'nominated',\n",
       " 'confidentiality',\n",
       " 'electoral',\n",
       " 'changelog',\n",
       " 'welding',\n",
       " 'orgasm',\n",
       " 'deferred',\n",
       " 'alternatively',\n",
       " 'heel',\n",
       " 'alloy',\n",
       " 'condo',\n",
       " 'plot',\n",
       " 'polished',\n",
       " 'yang',\n",
       " 'gently',\n",
       " 'greensboro',\n",
       " 'tulsa',\n",
       " 'locking',\n",
       " 'casey',\n",
       " 'controversial',\n",
       " 'draw',\n",
       " 'fridge',\n",
       " 'blanket',\n",
       " 'bloom',\n",
       " 'qc',\n",
       " 'simpson',\n",
       " 'lou',\n",
       " 'elliott',\n",
       " 'recovered',\n",
       " 'fraser',\n",
       " 'justify',\n",
       " 'upgrading',\n",
       " 'blade',\n",
       " 'pgp',\n",
       " 'loop',\n",
       " 'surge',\n",
       " 'frontpage',\n",
       " 'trauma',\n",
       " 'aw',\n",
       " 'tahoe',\n",
       " 'advert',\n",
       " 'posse',\n",
       " 'demanding',\n",
       " 'defensive',\n",
       " 'sip',\n",
       " 'flasher',\n",
       " 'subaru',\n",
       " 'forbidden',\n",
       " 'tf',\n",
       " 'vanilla',\n",
       " 'programmer',\n",
       " 'pj',\n",
       " 'monitored',\n",
       " 'installation',\n",
       " 'deutschland',\n",
       " 'picnic',\n",
       " 'soul',\n",
       " 'arrival',\n",
       " 'spank',\n",
       " 'cw',\n",
       " 'practitioner',\n",
       " 'motivated',\n",
       " 'wr',\n",
       " 'dumb',\n",
       " 'smithsonian',\n",
       " 'hollow',\n",
       " 'vault',\n",
       " 'securely',\n",
       " 'examining',\n",
       " 'fioricet',\n",
       " 'groove',\n",
       " 'revelation',\n",
       " 'rg',\n",
       " 'pursuit',\n",
       " 'delegation',\n",
       " 'wire',\n",
       " 'bl',\n",
       " 'dictionary',\n",
       " 'mail',\n",
       " 'backing',\n",
       " 'greenhouse',\n",
       " 'sleep',\n",
       " 'vc',\n",
       " 'blake',\n",
       " 'transparency',\n",
       " 'dee',\n",
       " 'travis',\n",
       " 'wx',\n",
       " 'endless',\n",
       " 'figured',\n",
       " 'orbit',\n",
       " 'currency',\n",
       " 'niger',\n",
       " 'bacon',\n",
       " 'survivor',\n",
       " 'positioning',\n",
       " 'heater',\n",
       " 'colony',\n",
       " 'cannon',\n",
       " 'circus',\n",
       " 'promoted',\n",
       " 'forbes',\n",
       " 'mae',\n",
       " 'moldova',\n",
       " 'mel',\n",
       " 'descending',\n",
       " 'paxil',\n",
       " 'spine',\n",
       " 'trout',\n",
       " 'enclosed',\n",
       " 'feat',\n",
       " 'temporarily',\n",
       " 'ntsc',\n",
       " 'cooked',\n",
       " 'thriller',\n",
       " 'transmit',\n",
       " 'apnic',\n",
       " 'fatty',\n",
       " 'gerald',\n",
       " 'pressed',\n",
       " 'frequency',\n",
       " 'scanned',\n",
       " 'reflection',\n",
       " 'hunger',\n",
       " 'mariah',\n",
       " 'sic',\n",
       " 'municipality',\n",
       " 'usps',\n",
       " 'joyce',\n",
       " 'detective',\n",
       " 'surgeon',\n",
       " 'cement',\n",
       " 'experiencing',\n",
       " 'fireplace',\n",
       " 'endorsement',\n",
       " 'bg',\n",
       " 'planner',\n",
       " 'dispute',\n",
       " 'textile',\n",
       " 'missile',\n",
       " 'intranet',\n",
       " 'close',\n",
       " 'seq',\n",
       " 'psychiatry',\n",
       " 'persistent',\n",
       " 'deborah',\n",
       " 'conf',\n",
       " 'marco',\n",
       " 'assist',\n",
       " 'summary',\n",
       " 'glow',\n",
       " 'gabriel',\n",
       " 'auditor',\n",
       " 'wma',\n",
       " 'aquarium',\n",
       " 'violin',\n",
       " 'prophet',\n",
       " 'cir',\n",
       " 'bracket',\n",
       " 'looksmart',\n",
       " 'isaac',\n",
       " 'oxide',\n",
       " 'oak',\n",
       " 'magnificent',\n",
       " 'erik',\n",
       " 'colleague',\n",
       " 'naples',\n",
       " 'promptly',\n",
       " 'modem',\n",
       " 'adaptation',\n",
       " 'hu',\n",
       " 'harmful',\n",
       " 'paintball',\n",
       " 'prozac',\n",
       " 'sexually',\n",
       " 'enclosure',\n",
       " 'acm',\n",
       " 'dividend',\n",
       " 'newark',\n",
       " 'kw',\n",
       " 'paso',\n",
       " 'glucose',\n",
       " 'phantom',\n",
       " 'norm',\n",
       " 'playback',\n",
       " 'supervisor',\n",
       " 'westminster',\n",
       " 'turtle',\n",
       " 'ip',\n",
       " 'distance',\n",
       " 'absorption',\n",
       " 'treasure',\n",
       " 'dsc',\n",
       " 'warned',\n",
       " 'neural',\n",
       " 'ware',\n",
       " 'fossil',\n",
       " 'mia',\n",
       " 'hometown',\n",
       " 'badly',\n",
       " 'transcript',\n",
       " 'apollo',\n",
       " 'wan',\n",
       " 'disappointed',\n",
       " 'persian',\n",
       " 'continually',\n",
       " 'communist',\n",
       " 'collectible',\n",
       " 'handmade',\n",
       " 'greene',\n",
       " 'entrepreneur',\n",
       " 'robot',\n",
       " 'grenada',\n",
       " 'creation',\n",
       " 'jade',\n",
       " 'scoop',\n",
       " 'acquisition',\n",
       " 'foul',\n",
       " 'keno',\n",
       " 'gtk',\n",
       " 'earning',\n",
       " 'mailman',\n",
       " 'sanyo',\n",
       " 'nested',\n",
       " 'biodiversity',\n",
       " 'excitement',\n",
       " 'somalia',\n",
       " 'mover',\n",
       " 'verbal',\n",
       " 'blink',\n",
       " 'presently',\n",
       " 'sea',\n",
       " 'carlo',\n",
       " 'workflow',\n",
       " 'mysterious',\n",
       " 'novelty',\n",
       " 'bryant',\n",
       " 'tile',\n",
       " 'voyuer',\n",
       " 'librarian',\n",
       " 'subsidiary',\n",
       " 'switched',\n",
       " 'stockholm',\n",
       " 'tamil',\n",
       " 'garmin',\n",
       " 'ru',\n",
       " 'pose',\n",
       " 'fuzzy',\n",
       " 'indonesian',\n",
       " 'gram',\n",
       " 'therapist',\n",
       " 'richards',\n",
       " 'mrna',\n",
       " 'budget',\n",
       " 'toolkit',\n",
       " 'promising',\n",
       " 'relaxation',\n",
       " 'goat',\n",
       " 'render',\n",
       " 'carmen',\n",
       " 'ira',\n",
       " 'sen',\n",
       " 'thereafter',\n",
       " 'hardwood',\n",
       " 'erotica',\n",
       " 'temporal',\n",
       " 'sail',\n",
       " 'forge',\n",
       " 'commissioner',\n",
       " 'dense',\n",
       " 'dts',\n",
       " 'brave',\n",
       " 'forwarding',\n",
       " 'qt',\n",
       " 'awful',\n",
       " 'nightmare',\n",
       " 'airplane',\n",
       " 'reduction',\n",
       " 'southampton',\n",
       " 'istanbul',\n",
       " 'impose',\n",
       " 'organism',\n",
       " 'sega',\n",
       " 'telescope',\n",
       " 'viewer',\n",
       " 'asbestos',\n",
       " 'portsmouth',\n",
       " 'cdna',\n",
       " 'meyer',\n",
       " 'enters',\n",
       " 'pod',\n",
       " 'savage',\n",
       " 'advancement',\n",
       " 'wu',\n",
       " 'harassment',\n",
       " 'willow',\n",
       " 'resume',\n",
       " 'bolt',\n",
       " 'gage',\n",
       " 'throwing',\n",
       " 'existed',\n",
       " 'whore',\n",
       " 'generator',\n",
       " 'lu',\n",
       " 'wagon',\n",
       " 'barbie',\n",
       " 'dat',\n",
       " 'favor',\n",
       " 'soa',\n",
       " 'knock',\n",
       " 'urge',\n",
       " 'smtp',\n",
       " 'generates',\n",
       " 'potato',\n",
       " 'thorough',\n",
       " 'replication',\n",
       " 'inexpensive',\n",
       " 'kurt',\n",
       " 'receptor',\n",
       " 'peer',\n",
       " 'roland',\n",
       " 'optimum',\n",
       " 'neon',\n",
       " 'intervention',\n",
       " 'quilt',\n",
       " 'huntington',\n",
       " 'creature',\n",
       " 'ours',\n",
       " 'mount',\n",
       " 'syracuse',\n",
       " 'internship',\n",
       " 'lone',\n",
       " 'refresh',\n",
       " 'aluminium',\n",
       " 'snowboard',\n",
       " 'beastality',\n",
       " 'webcast',\n",
       " 'michel',\n",
       " 'evanescence',\n",
       " 'subtle',\n",
       " 'coordinated',\n",
       " 'notre',\n",
       " 'shipment',\n",
       " 'maldives',\n",
       " 'stripe',\n",
       " 'firmware',\n",
       " 'antarctica',\n",
       " 'cope',\n",
       " 'shepherd',\n",
       " 'lm',\n",
       " 'canberra',\n",
       " 'cradle',\n",
       " 'chancellor',\n",
       " 'mambo',\n",
       " 'lime',\n",
       " 'kirk',\n",
       " 'flour',\n",
       " 'controversy',\n",
       " 'legendary',\n",
       " 'bool',\n",
       " 'sympathy',\n",
       " 'choir',\n",
       " 'avoiding',\n",
       " 'beautifully',\n",
       " 'blond',\n",
       " 'expects',\n",
       " 'cho',\n",
       " 'jumping',\n",
       " 'fabric',\n",
       " 'antibody',\n",
       " 'polymer',\n",
       " 'hygiene',\n",
       " 'wit',\n",
       " 'poultry',\n",
       " 'virtue',\n",
       " 'burst',\n",
       " 'examination',\n",
       " 'surgeon',\n",
       " 'bouquet',\n",
       " 'immunology',\n",
       " 'promotes',\n",
       " 'mandate',\n",
       " 'wiley',\n",
       " 'departmental',\n",
       " 'bb',\n",
       " 'spa',\n",
       " 'ind',\n",
       " 'corpus',\n",
       " 'johnston',\n",
       " 'terminology',\n",
       " 'gentleman',\n",
       " 'fibre',\n",
       " 'reproduce',\n",
       " 'convicted',\n",
       " 'shade',\n",
       " 'jet',\n",
       " 'index',\n",
       " 'roommate',\n",
       " 'adware',\n",
       " 'qui',\n",
       " 'intl',\n",
       " 'threatening',\n",
       " 'spokesman',\n",
       " 'zoloft',\n",
       " 'activist',\n",
       " 'frankfurt',\n",
       " 'prisoner',\n",
       " 'daisy',\n",
       " 'halifax',\n",
       " 'encourages',\n",
       " 'ultram',\n",
       " 'cursor',\n",
       " 'assembled',\n",
       " 'earliest',\n",
       " 'donated',\n",
       " 'stuffed',\n",
       " 'restructuring',\n",
       " 'insect',\n",
       " 'terminal',\n",
       " 'crude',\n",
       " 'morrison',\n",
       " 'maiden',\n",
       " 'simulation',\n",
       " 'cz',\n",
       " 'sufficiently',\n",
       " 'examines',\n",
       " 'viking',\n",
       " 'myrtle',\n",
       " 'bored',\n",
       " 'cleanup',\n",
       " 'yarn',\n",
       " 'knit',\n",
       " 'conditional',\n",
       " 'mug',\n",
       " 'crossword',\n",
       " 'bother',\n",
       " 'budapest',\n",
       " 'conceptual',\n",
       " 'knitting',\n",
       " 'attacked',\n",
       " 'hl',\n",
       " 'bhutan',\n",
       " 'liechtenstein',\n",
       " 'mating',\n",
       " 'compute',\n",
       " 'redhead',\n",
       " 'arrives',\n",
       " 'translator',\n",
       " 'automobile',\n",
       " 'tractor',\n",
       " 'allah',\n",
       " 'continent',\n",
       " 'ob',\n",
       " 'unwrap',\n",
       " 'fare',\n",
       " 'longitude',\n",
       " 'resist',\n",
       " 'challenged',\n",
       " 'telecharger',\n",
       " 'hoped',\n",
       " 'pike',\n",
       " 'safer',\n",
       " 'insertion',\n",
       " 'instrumentation',\n",
       " 'id',\n",
       " 'hugo',\n",
       " 'wagner',\n",
       " 'constraint',\n",
       " 'groundwater',\n",
       " 'touched',\n",
       " 'strengthening',\n",
       " 'cologne',\n",
       " 'gzip',\n",
       " 'wishing',\n",
       " 'ranger',\n",
       " 'smallest',\n",
       " 'insulation',\n",
       " 'newman',\n",
       " 'marsh',\n",
       " 'ricky',\n",
       " 'ctrl',\n",
       " 'scared',\n",
       " 'theta',\n",
       " 'infringement',\n",
       " 'bent',\n",
       " 'lao',\n",
       " 'subjective',\n",
       " 'monster',\n",
       " 'asylum',\n",
       " 'lightbox',\n",
       " 'robbie',\n",
       " 'stake',\n",
       " 'cocktail',\n",
       " 'outlet',\n",
       " 'swaziland',\n",
       " 'variety',\n",
       " 'arbor',\n",
       " 'mediawiki',\n",
       " 'configuration',\n",
       " 'poison']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmas = []\n",
    "# for word in cwordlist: \n",
    "#     lemmas.append(wl.lemmatize(word))\n",
    "# lemmas[9030:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #find unique lemmas \n",
    "# len(lemmas)\n",
    "# unique_lemmas = set(lemmas)\n",
    "# len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pichunter',\n",
       " 'xnxx',\n",
       " 'blowjob',\n",
       " 'worldsex',\n",
       " 'twinks',\n",
       " 'viewpicture',\n",
       " 'qty',\n",
       " 'bufing',\n",
       " 'mailto',\n",
       " 'bangbus',\n",
       " 'verzeichnis',\n",
       " 'uniprotkb',\n",
       " 'beastiality',\n",
       " 'sublimedirectory',\n",
       " 'handjobs',\n",
       " 'diffs',\n",
       " 'smilies',\n",
       " 'gratuit',\n",
       " 'milfhunter',\n",
       " 'trembl',\n",
       " 'milfs',\n",
       " 'findarticles',\n",
       " 'sbjct',\n",
       " 'voyeurweb',\n",
       " 'handjob',\n",
       " 'shemale',\n",
       " 'knowledgestorm',\n",
       " 'jelsoft',\n",
       " 'cooky',\n",
       " 'trackback',\n",
       " 'sexcam',\n",
       " 'msgid',\n",
       " 'shemales',\n",
       " 'expansys',\n",
       " 'powerseller',\n",
       " 'voyuer',\n",
       " 'transexuales',\n",
       " 'basename',\n",
       " 'endif',\n",
       " 'dealtime',\n",
       " 'vsnet',\n",
       " 'listprice',\n",
       " 'ampland',\n",
       " 'msie',\n",
       " 'livecam',\n",
       " 'upskirts',\n",
       " 'titten',\n",
       " 'upskirt',\n",
       " 'feof',\n",
       " 'livesex',\n",
       " 'postposted',\n",
       " 'prostores',\n",
       " 'fioricet',\n",
       " 'href',\n",
       " 'holdem',\n",
       " 'msgstr',\n",
       " 'trackbacks',\n",
       " 'telecharger',\n",
       " 'ultram',\n",
       " 'acdbentity',\n",
       " 'techrepublic',\n",
       " 'adipex',\n",
       " 'cumshot',\n",
       " 'changelog',\n",
       " 'starsmerchant',\n",
       " 'thumbzilla',\n",
       " 'thehun',\n",
       " 'beastality',\n",
       " 'cumshots',\n",
       " 'permalink']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #make sure all lemmas are in the vocabulary\n",
    "# valid_lemmas = []\n",
    "# invalid_lemmas = [] \n",
    "\n",
    "# for word in unique_lemmas:\n",
    "#     if word in model.keys():\n",
    "#         valid_lemmas.append(word)\n",
    "#     else:\n",
    "#         invalid_lemmas.append(word)\n",
    "\n",
    "# invalid_lemmas \n",
    "# #mostly little-used porn words so i'm fine with them being excluded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #find small lemmas to make sure they are valid words \n",
    "# lemmas_df = pd.DataFrame(valid_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8350"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmas_df.columns = ['lemma']\n",
    "# lemmas_df['lemma'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>mn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>dj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>wt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>nu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>pi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>bm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>ad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>yo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma\n",
       "35     de\n",
       "44     mn\n",
       "55     ic\n",
       "64     dj\n",
       "86     wt\n",
       "106    nu\n",
       "123    pi\n",
       "147    te\n",
       "153    us\n",
       "191    bm\n",
       "195    eq\n",
       "197    af\n",
       "207    ad\n",
       "229    ph\n",
       "238    yo"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# short_lemmas = lemmas_df[lemmas_df['lemma'].apply(lambda x: len(x) < 3)]\n",
    "# len(short_lemmas)\n",
    "# short_lemmas.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want my bot to start with any of these unknown, smaller words, and it likely won't use them very much. Rather than limiting the bot's response, since these are supposedly the top 8000 most commonly used words, I am going to limit my bot's initial \"random\" response to be within the first 2000 common words to avoid weird starts to the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #get vectors for lemmas \n",
    "# common_word_vectors = {}\n",
    "# unvectorized_common_words = []\n",
    "# for word in valid_lemmas:\n",
    "#     if word in model.keys():\n",
    "#         common_word_vectors[word] = model[word]\n",
    "#     else:\n",
    "#         unvectorized_common_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #reshape the dictionary arrays \n",
    "# common_word_vectors['truth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for key, value in common_word_vectors.items():\n",
    "#     common_word_vectors[key] = value.reshape(1,-1)\n",
    "\n",
    "# common_word_vectors['truth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pickle the common word vectors \n",
    "# file_name = 'common_word_vectors.pkl'\n",
    "# pickle.dump(common_word_vectors, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_word_vectors = pickle.load(open('common_word_vectors.pkl', 'rb'))\n",
    "common_word_vectors['the'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converge(user_input=str, bot_input=str, exclude=None):\n",
    "    \"\"\"Return the \"average word\" of the input words.\"\"\"\n",
    "    if exclude is None:\n",
    "        exclude = set()\n",
    "    exclude.add(user_input)\n",
    "    exclude.add(bot_input)  \n",
    "\n",
    "    mean_vector = ((model[user_input] + model[bot_input])/2)\n",
    "    cos_sim_dict = {}\n",
    "    response_options_dict = {key: common_word_vectors[key] for key in common_word_vectors \n",
    "                             if key not in exclude}\n",
    "    for word, vector in response_options_dict.items():\n",
    "        cos_sim_dict[float(cosine_similarity(mean_vector, vector))] = word\n",
    "    max_cos_sim = max(cos_sim_dict.keys())\n",
    "    bot_response = cos_sim_dict[max_cos_sim]\n",
    "    return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_round(user_input, user_history=None, bot_history=None):\n",
    "    \"\"\"Play a round of Convergence.\"\"\"\n",
    "    if bot_history is None:\n",
    "        user_history = []\n",
    "        bot_history = []\n",
    "        bot_response = random.choice(valid_lemmas[:2000])\n",
    "    else:\n",
    "        bot_response = converge(user_history[-1], bot_history[-1], \n",
    "                                exclude=set(user_history + bot_history))\n",
    "    user_history.append(user_input)\n",
    "    bot_history.append(bot_response)\n",
    "    return {\n",
    "        'user_history': user_history,\n",
    "        'bot_history': bot_history,\n",
    "        'bot_response': bot_response,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_convergence(round_results=None):\n",
    "    \"\"\"Play a game of Convergence.\"\"\"\n",
    "    print(\"Let's play Convergence. Type any word!\") \n",
    "    while True: \n",
    "        user_response = input()\n",
    "        if round_results is None:\n",
    "            bot_history = None \n",
    "            user_history = None\n",
    "            round_results = play_round(user_response, user_history, bot_history)    \n",
    "            #this will practically never happen, but just in case: \n",
    "            if user_response != round_results['bot_response']:\n",
    "                print(f\"You said {user_response}, but I said {round_results['bot_response']}.\")\n",
    "                print(f\"What's the convergence of {user_response} and {round_results['bot_response']}?\")   \n",
    "\n",
    "            else:\n",
    "                print(f\"CONVERGENCE!!!! WE BOTH said {user_response}!!! WE GOT CONVERGENCE!!!!\")\n",
    "                break              \n",
    "        else: \n",
    "            bot_history = round_results['bot_history']\n",
    "            user_history = round_results['user_history']\n",
    "            \n",
    "            #ensure user_response is a real word: \n",
    "            if user_response in bot_history or user_response in user_history: \n",
    "                print(f\"Looks like one of us has already said {user_response}. Choose a different word.\")\n",
    "            elif user_response in model.keys():  \n",
    "\n",
    "                round_results = play_round(user_response, user_history, bot_history)    \n",
    "                if user_response != round_results['bot_response']:\n",
    "                    print(f\"You said {user_response}, but I said {round_results['bot_response']}.\")\n",
    "                    print(f\"What's the convergence of {user_response} and {round_results['bot_response']}?\")   \n",
    "                    if len(round_results['bot_history'])>1:\n",
    "                          print(f\"Remember, don't repeat what you said: {user_history} or what I said: {bot_history}\")\n",
    "                else:\n",
    "                    print(f\"CONVERGENCE!!!! WE BOTH said {user_response}!!! WE GOT CONVERGENCE!!!!\")\n",
    "                    break              \n",
    "            else: \n",
    "                print (f\"Is {user_response} an English word? I don't know it. Choose a different word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play Convergence. Type any word!\n",
      "hello\n",
      "You said hello, but I said arrangement.\n",
      "What's the convergence of hello and arrangement?\n",
      "flower\n",
      "You said flower, but I said hey.\n",
      "What's the convergence of flower and hey?\n",
      "Remember, don't repeat what you said: ['hello', 'flower'] or what I said: ['arrangement', 'hey']\n",
      "hay\n",
      "You said hay, but I said lovely.\n",
      "What's the convergence of hay and lovely?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay'] or what I said: ['arrangement', 'hey', 'lovely']\n",
      "farm\n",
      "You said farm, but I said beautiful.\n",
      "What's the convergence of farm and beautiful?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful']\n",
      "erger\n",
      "Is erger an English word? I don't know it. Choose a different word.\n",
      "farm\n",
      "Looks like one of us has already said farm. Choose a different word.\n",
      "beatuiful\n",
      "Is beatuiful an English word? I don't know it. Choose a different word.\n",
      "beautiful\n",
      "Looks like one of us has already said beautiful. Choose a different word.\n",
      "pasture\n",
      "You said pasture, but I said gorgeous.\n",
      "What's the convergence of pasture and gorgeous?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous']\n",
      "scenery\n",
      "You said scenery, but I said vegetation.\n",
      "What's the convergence of scenery and vegetation?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture', 'scenery'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous', 'vegetation']\n",
      "trees\n",
      "You said trees, but I said terrain.\n",
      "What's the convergence of trees and terrain?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture', 'scenery', 'trees'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous', 'vegetation', 'terrain']\n",
      "forest\n",
      "You said forest, but I said tree.\n",
      "What's the convergence of forest and tree?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture', 'scenery', 'trees', 'forest'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous', 'vegetation', 'terrain', 'tree']\n",
      "forestry\n",
      "You said forestry, but I said pine.\n",
      "What's the convergence of forestry and pine?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture', 'scenery', 'trees', 'forest', 'forestry'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous', 'vegetation', 'terrain', 'tree', 'pine']\n",
      "needle\n",
      "You said needle, but I said timber.\n",
      "What's the convergence of needle and timber?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture', 'scenery', 'trees', 'forest', 'forestry', 'needle'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous', 'vegetation', 'terrain', 'tree', 'pine', 'timber']\n",
      "evergreen\n",
      "You said evergreen, but I said logging.\n",
      "What's the convergence of evergreen and logging?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture', 'scenery', 'trees', 'forest', 'forestry', 'needle', 'evergreen'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous', 'vegetation', 'terrain', 'tree', 'pine', 'timber', 'logging']\n",
      "lumber\n",
      "You said lumber, but I said hardwood.\n",
      "What's the convergence of lumber and hardwood?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture', 'scenery', 'trees', 'forest', 'forestry', 'needle', 'evergreen', 'lumber'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous', 'vegetation', 'terrain', 'tree', 'pine', 'timber', 'logging', 'hardwood']\n",
      "floors\n",
      "You said floors, but I said flooring.\n",
      "What's the convergence of floors and flooring?\n",
      "Remember, don't repeat what you said: ['hello', 'flower', 'hay', 'farm', 'pasture', 'scenery', 'trees', 'forest', 'forestry', 'needle', 'evergreen', 'lumber', 'floors'] or what I said: ['arrangement', 'hey', 'lovely', 'beautiful', 'gorgeous', 'vegetation', 'terrain', 'tree', 'pine', 'timber', 'logging', 'hardwood', 'flooring']\n",
      "floor\n",
      "CONVERGENCE!!!! WE BOTH said floor!!! WE GOT CONVERGENCE!!!!\n"
     ]
    }
   ],
   "source": [
    "play_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
